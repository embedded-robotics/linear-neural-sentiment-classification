{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\muhammadawais.naeem\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from utils import *\n",
    "from sentiment_data import *\n",
    "from sentiment_classifier import *\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exs = read_sentiment_examples(\"data/train.txt\")\n",
    "dev_exs = read_sentiment_examples(\"data/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 14923 vectors of size 300\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = read_word_embeddings(\"data/glove.6B.300d-relativized.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_classes, word_embeddings):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.embed = word_embeddings.get_initialized_embedding_layer()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_layers)\n",
    "        self.linear2 = nn.Linear(hidden_layers, 64)\n",
    "        self.linear3 = nn.Linear(64, output_classes)\n",
    "        # Initialize weights according to a formula due to Xavier Glorot.\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.embed(input_data)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 296.9442\n",
      "Epoch 2/50, Loss: 292.5028\n",
      "Epoch 3/50, Loss: 281.8027\n",
      "Epoch 4/50, Loss: 268.5917\n",
      "Epoch 5/50, Loss: 262.5089\n",
      "Epoch 6/50, Loss: 255.5067\n",
      "Epoch 7/50, Loss: 238.7361\n",
      "Epoch 8/50, Loss: 230.0239\n",
      "Epoch 9/50, Loss: 216.9828\n",
      "Epoch 10/50, Loss: 209.5482\n",
      "Epoch 11/50, Loss: 211.5334\n",
      "Epoch 12/50, Loss: 219.0288\n",
      "Epoch 13/50, Loss: 200.4082\n",
      "Epoch 14/50, Loss: 212.5712\n",
      "Epoch 15/50, Loss: 203.6005\n",
      "Epoch 16/50, Loss: 198.7287\n",
      "Epoch 17/50, Loss: 208.8603\n",
      "Epoch 18/50, Loss: 200.0419\n",
      "Epoch 19/50, Loss: 201.3248\n",
      "Epoch 20/50, Loss: 196.4376\n",
      "Epoch 21/50, Loss: 197.1770\n",
      "Epoch 22/50, Loss: 202.5271\n",
      "Epoch 23/50, Loss: 206.6694\n",
      "Epoch 24/50, Loss: 200.4968\n",
      "Epoch 25/50, Loss: 208.2820\n",
      "Epoch 26/50, Loss: 190.6926\n",
      "Epoch 27/50, Loss: 213.7326\n",
      "Epoch 28/50, Loss: 186.9994\n",
      "Epoch 29/50, Loss: 198.2222\n",
      "Epoch 30/50, Loss: 205.5710\n",
      "Epoch 31/50, Loss: 203.9734\n",
      "Epoch 32/50, Loss: 205.1412\n",
      "Epoch 33/50, Loss: 187.0697\n",
      "Epoch 34/50, Loss: 201.7696\n",
      "Epoch 35/50, Loss: 196.5718\n",
      "Epoch 36/50, Loss: 171.9337\n",
      "Epoch 37/50, Loss: 186.6661\n",
      "Epoch 38/50, Loss: 186.6661\n",
      "Epoch 39/50, Loss: 200.2843\n",
      "Epoch 40/50, Loss: 174.9625\n",
      "Epoch 41/50, Loss: 185.5732\n",
      "Epoch 42/50, Loss: 194.8682\n",
      "Epoch 43/50, Loss: 187.3277\n",
      "Epoch 44/50, Loss: 199.3034\n",
      "Epoch 45/50, Loss: 206.0932\n",
      "Epoch 46/50, Loss: 174.5030\n",
      "Epoch 47/50, Loss: 184.9369\n",
      "Epoch 48/50, Loss: 179.3162\n",
      "Epoch 49/50, Loss: 193.9893\n",
      "Epoch 50/50, Loss: 216.1025\n"
     ]
    }
   ],
   "source": [
    "# RUN TRAINING AND TEST\n",
    "pad_length=40\n",
    "num_epochs = 50\n",
    "input_size = 300\n",
    "hidden_layers = 128\n",
    "output_classes = 2\n",
    "batch_size = 16\n",
    "total_samples = len(train_exs)\n",
    "batch_indices = np.arange(0, total_samples, batch_size)\n",
    "\n",
    "ffnn = FFNN(input_size, hidden_layers, output_classes, word_embeddings)\n",
    "initial_learning_rate = 0.0001\n",
    "optimizer = optim.Adam(ffnn.parameters(), lr=initial_learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    random.seed(42)\n",
    "    random.shuffle(train_exs)\n",
    "    \n",
    "    for i in range(0, len(batch_indices)-1):\n",
    "        train_batch = train_exs[i:i+1]\n",
    "        word_indices = []\n",
    "        labels = []\n",
    "        # Pad or truncate each sample in the current batch\n",
    "        for train_sample in train_batch:\n",
    "            word_list = train_sample.words\n",
    "            index = []\n",
    "            for word in word_list:\n",
    "                word_index = word_embeddings.word_indexer.index_of(word)\n",
    "                if word_index != -1:\n",
    "                    index.append(word_index)\n",
    "                else:\n",
    "                    index.append(0)\n",
    "            \n",
    "            # Pad or truncate the index\n",
    "            index_length = len(index)\n",
    "            if index_length > pad_length:\n",
    "                index = index[0:pad_length]\n",
    "            else:\n",
    "                for i in range(index_length, pad_length):\n",
    "                    index.append(1)\n",
    "            \n",
    "            word_indices.append(index)\n",
    "            labels.append([train_sample.label])\n",
    "\n",
    "        x = torch.from_numpy(np.array(word_indices)).int()\n",
    "        y = train_sample.label\n",
    "        \n",
    "        # # Build one-hot representation of y. Instead of the label 0 or 1, y_onehot is either [0, 1] or [1, 0]. This\n",
    "        # # way we can take the dot product directly with a probability vector to get class probabilities.\n",
    "        y_onehot = torch.zeros((len(labels), output_classes))\n",
    "        # # scatter will write the value of 1 into the position of y_onehot given by y\n",
    "        y_onehot.scatter_(1, torch.from_numpy(np.asarray(labels, dtype=np.int64)), 1)\n",
    "\n",
    "        # Zero out the gradients from the FFNN object. *THIS IS VERY IMPORTANT TO DO BEFORE CALLING BACKWARD()*\n",
    "        optimizer.zero_grad()\n",
    "        output = ffnn(x)\n",
    "        # Can also use built-in NLLLoss as a shortcut here but we're being explicit here\n",
    "        loss = criterion(output, y_onehot)\n",
    "        # Computes the gradient and takes the optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "total_samples = len(train_exs)\n",
    "batch_indices = np.arange(0, total_samples, batch_size)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(0, len(batch_indices)-1):\n",
    "    train_batch = train_exs[i:i+1]\n",
    "    word_indices = []\n",
    "    # Pad or truncate each sample in the current batch\n",
    "    for train_sample in train_batch:\n",
    "        word_list = train_sample.words\n",
    "        index = []\n",
    "        for word in word_list:\n",
    "            word_index = word_embeddings.word_indexer.index_of(word)\n",
    "            if word_index != -1:\n",
    "                index.append(word_index)\n",
    "            else:\n",
    "                index.append(1) #unknown 'UAK'\n",
    "        \n",
    "        # Pad or truncate the index\n",
    "        index_length = len(index)\n",
    "        if index_length > pad_length:\n",
    "            index = index[0:pad_length]\n",
    "        else:\n",
    "            for i in range(index_length, pad_length):\n",
    "                index.append(0)\n",
    "        \n",
    "        word_indices.append(index)\n",
    "        y_true.append(train_sample.label)\n",
    "\n",
    "    x = torch.from_numpy(np.array(word_indices)).int()\n",
    "    output = ffnn(x)\n",
    "    y_pred.append(torch.argmax(output).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.75      0.77       204\n",
      "           1       0.79      0.83      0.81       228\n",
      "\n",
      "    accuracy                           0.79       432\n",
      "   macro avg       0.79      0.79      0.79       432\n",
      "weighted avg       0.79      0.79      0.79       432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "total_samples = len(dev_exs)\n",
    "batch_indices = np.arange(0, total_samples, batch_size)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(0, len(batch_indices)-1):\n",
    "    train_batch = dev_exs[i:i+1]\n",
    "    word_indices = []\n",
    "    # Pad or truncate each sample in the current batch\n",
    "    for train_sample in train_batch:\n",
    "        word_list = train_sample.words\n",
    "        index = []\n",
    "        for word in word_list:\n",
    "            word_index = word_embeddings.word_indexer.index_of(word)\n",
    "            if word_index != -1:\n",
    "                index.append(word_index)\n",
    "            else:\n",
    "                index.append(0)\n",
    "        \n",
    "        # Pad or truncate the index\n",
    "        index_length = len(index)\n",
    "        if index_length > pad_length:\n",
    "            index = index[0:pad_length]\n",
    "        else:\n",
    "            for i in range(index_length, pad_length):\n",
    "                index.append(1)\n",
    "        \n",
    "        word_indices.append(index)\n",
    "        y_true.append(train_sample.label)\n",
    "\n",
    "    x = torch.from_numpy(np.array(word_indices)).int()\n",
    "    output = ffnn(x)\n",
    "    y_pred.append(torch.argmax(output).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.67      0.29         3\n",
      "           1       0.98      0.82      0.89        51\n",
      "\n",
      "    accuracy                           0.81        54\n",
      "   macro avg       0.58      0.75      0.59        54\n",
      "weighted avg       0.93      0.81      0.86        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
